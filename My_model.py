# -*- coding: utf-8 -*-
"""DWT+PCA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/116THRoxeBBidhyQ9dWJ4Cpi-k-MT7eTf

PART 1

    1. Preprocessing
    2. oversampling (balance the data) : SMOTE
    3. Feature Scaling : StandardScaler
    4. Feature Extraction : DWT
    5. Dimensionality reduction : PCA
    6. Model training : XGBoost, KNN, DT, RF and MLP
    7. Hard voting ()
    8. Evaluation metrices : accuracy, precision, recall and F1 score

### Classifier (XGBoost, KNN, DT, RF and MLP ) : DWT + PCA
"""

!pip install PyWavelets

import pandas as pd                                      #dataframe
import numpy as np                                       #array
import pywt                                              #feature extraction
from sklearn.preprocessing import StandardScaler         #scaling
from sklearn.model_selection import train_test_split     #split the dataset
import matplotlib.pyplot as plt                          #visualization
import seaborn as sns                                    #visualization
from sklearn.metrics import confusion_matrix             #confusion matrix
from imblearn.over_sampling import SMOTE                 #oversampling
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay                    #confusion matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score     #parameters
from sklearn.decomposition import PCA                    #dimensionality reduction
from sklearn.manifold import TSNE                       #dimensionality reduction
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
import joblib
from sklearn.ensemble import VotingClassifier

data= pd.read_csv("/content/data.csv")
data.head(2)

"""### STEP1. Pre-Processing"""

print("Dataset Information:")
print(data.info())

missing_values = data.isnull().sum().sum()
print("\nMissing Values in dataset:", missing_values)


null_values = data.isna().sum().sum()
print("\nNull Values in dataset:", null_values)

"""### STEP2. Oversampling"""

X = data.drop(columns=['column_a', 'y'])
y = data['y']

# Combine normal conditions (2, 3, 4, 5) into a single class (0)
y_combined = y.copy()
y_combined[y_combined != 1] = 0  # Class 1 is epileptic, others become class 0

# Perform train-test split (optional, for evaluation purposes)
X_train, X_test, y_train, y_test = train_test_split(X, y_combined, test_size=0.2, random_state=42)

# Apply SMOTE to oversample the minority class (epileptic condition)
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Now X_train_resampled and y_train_resampled have balanced classes
print(f'Original dataset shape: {X_train.shape}')
print(f'Resampled dataset shape: {X_train_resampled.shape}')
print(f'Class distribution after SMOTE: {pd.Series(y_train_resampled).value_counts()}')

"""### STEP3. Feature Scaling"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)

# Now X_resampled_scaled and X_test_scaled are ready for model training
print(f'Scaled Resampled Data Shape: {X_train_scaled.shape}')
print(f'Test Data Shape: {X_test_scaled.shape}')

"""### STEP4. Feature Extraction using DWT"""

def extract_dwt_features(data):
    # Apply DWT and extract features
    features = []
    for signal in data:
        coeffs = pywt.wavedec(signal, wavelet='db4', level=5)
        flattened_coeffs = np.hstack(coeffs)
        features.append(flattened_coeffs)
    return np.array(features)

# Extract DWT features from the scaled training data
X_train_dwt = extract_dwt_features(X_train_scaled)

# Optional: Extract DWT features from the test data (scaled)
X_test_scaled = scaler.transform(X_test)
X_test_dwt = extract_dwt_features(X_test_scaled)

# Now X_resampled_dwt and X_test_dwt are ready for further processing
print(f'DWT Feature Shape for Resampled Data: {X_train_dwt.shape}')
print(f'DWT Feature Shape for Test Data: {X_test_dwt.shape}')

"""### STEP5. Dimensionality Reduction"""

pca = PCA(n_components=0.95)  # Choose components that explain 95% of variance
X_train_pca = pca.fit_transform(X_train_dwt)

# Optional: Apply PCA to the test data (scaled and DWT features)
X_test_scaled = scaler.transform(X_test)
X_test_dwt = extract_dwt_features(X_test_scaled)
X_test_pca = pca.transform(X_test_dwt)

# Now X_resampled_pca and X_test_pca are ready for model training
print(f'PCA Transformed Shape for Resampled Data: {X_train_pca.shape}')
print(f'PCA Transformed Shape for Test Data: {X_test_pca.shape}')

"""### STEP6. Model Training"""

classifiers = {
    'XGBoost': XGBClassifier(eval_metric='mlogloss', use_label_encoder=False),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'MLP': MLPClassifier(max_iter=500)
}

# Train and evaluate each classifier
for name, clf in classifiers.items():
    # Train the model
    clf.fit(X_train_pca, y_train_resampled)

    # Predict on the test data
    y_pred = clf.predict(X_test_pca)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Display results
    print(f"\nClassifier: {name}")
    print(f"Confusion Matrix:\n{cm}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(classification_report(y_test, y_pred))

"""Soft voting ensemble method se karke dekh lia (accuracy= 0.9809), ab HARD voting (0.9817) se dekhte hai

### soft voting

clf1 = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)
clf2 = KNeighborsClassifier()
clf3 = DecisionTreeClassifier()
clf4 = RandomForestClassifier()
clf5 = MLPClassifier(max_iter=500)

# Create a voting classifier with soft voting
voting_clf = VotingClassifier(estimators=[
    ('XGBoost', clf1),
    ('KNN', clf2),
    ('Decision Tree', clf3),
    ('Random Forest', clf4),
    ('MLP', clf5)],
    voting='soft'
)

# Train the voting classifier
voting_clf.fit(X_train_pca, y_train_resampled)

# Predict on the test data
y_pred = voting_clf.predict(X_test_pca)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Display results
print(f"\nVoting Classifier (Soft) Results:")
print(f"Confusion Matrix:\n{cm}")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(classification_report(y_test, y_pred))
###
"""

clf1 = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)
clf2 = KNeighborsClassifier()
clf3 = DecisionTreeClassifier()
clf4 = RandomForestClassifier()
clf5 = MLPClassifier(max_iter=500)

# Create a voting classifier with hard voting
voting_clf = VotingClassifier(estimators=[
    ('XGBoost', clf1),
    ('KNN', clf2),
    ('Decision Tree', clf3),
    ('Random Forest', clf4),
    ('MLP', clf5)],
    voting='hard'  # Use hard voting
)

# Train the voting classifier
voting_clf.fit(X_train_pca, y_train_resampled)

# Predict on the test data
y_pred = voting_clf.predict(X_test_pca)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Display results
print(f"\nVoting Classifier (Hard) Results:")
print(f"Confusion Matrix:\n{cm}")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(classification_report(y_test, y_pred))

print(f"Decision Tree training data shape: {X_train.shape}")
print(f"Random Forest training data shape: {X_train.shape}")

import pickle
import os
model_filename = 'Model.pkl'
with open(model_filename, 'wb') as model_file:
    pickle.dump(voting_clf, model_file)  #using hard filtering as of now
absolute_path = os.path.abspath(model_filename)

# Print the confirmation message and the path
print("Model saved successfully!")
print("Model file path:", absolute_path)
